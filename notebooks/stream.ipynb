{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc6a911-7718-4681-b489-fd0c9bd1d8b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One or more environment variables are missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m tf_var_tenant_id \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF_VAR_TENANT_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m     15\u001b[0m     [\n\u001b[1;32m     16\u001b[0m         storage_account_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     ]\n\u001b[1;32m     22\u001b[0m ):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more environment variables are missing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Configure Spark session\u001b[39;00m\n\u001b[1;32m     26\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     27\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncrementalProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.parquet.int96RebaseModeInWrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEGACY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     30\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: One or more environment variables are missing"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, approx_count_distinct, avg, max, min, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Environment variables for Azure Data Lake Storage\n",
    "storage_account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")\n",
    "storage_container_name = os.getenv(\"STORAGE_CONTAINER_NAME\")\n",
    "tf_var_client_id = os.getenv(\"TF_VAR_CLIENT_ID\")\n",
    "tf_var_client_secret = os.getenv(\"TF_VAR_CLIENT_SECRET\")\n",
    "tf_var_tenant_id = os.getenv(\"TF_VAR_TENANT_ID\")\n",
    "\n",
    "if not all(\n",
    "    [\n",
    "        storage_account_name,\n",
    "        storage_container_name,\n",
    "        tf_var_client_id,\n",
    "        tf_var_client_secret,\n",
    "        tf_var_tenant_id,\n",
    "    ]\n",
    "):\n",
    "    raise ValueError(\"One or more environment variables are missing\")\n",
    "\n",
    "# Configure Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"IncrementalProcessing\")\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"LEGACY\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\",\n",
    "    \"OAuth\",\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\",\n",
    "    tf_var_client_id,\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\",\n",
    "    tf_var_client_secret,\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\",\n",
    "    f\"https://login.microsoftonline.com/{tf_var_tenant_id}/oauth2/token\",\n",
    ")\n",
    "\n",
    "# Load Parquet files from ADLS\n",
    "incremental_hotel_weather_path = f\"abfss://{storage_container_name}@{storage_account_name}.dfs.core.windows.net/incremental-hotel-weather\"\n",
    "df = spark.read.format(\"parquet\").parquet(incremental_hotel_weather_path)\n",
    "\n",
    "# Show the loaded DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8a8a6e-4a92-447d-a36e-7090865bbb1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['city, 'wthr_date], ['city, 'wthr_date, approx_count_distinct('id, 0.05, 0, 0) AS distinct_hotels#467, avg('avg_tmpr_c) AS avg_temperature#469, max('avg_tmpr_c) AS max_temperature#471, min('avg_tmpr_c) AS min_temperature#473]\n",
      "+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1a77144e,cloudFiles,List(),Some(StructType(StructField(address,StringType,true),StructField(avg_tmpr_c,DoubleType,true),StructField(avg_tmpr_f,DoubleType,true),StructField(city,StringType,true),StructField(country,StringType,true),StructField(geoHash,StringType,true),StructField(id,StringType,true),StructField(latitude,DoubleType,true),StructField(longitude,DoubleType,true),StructField(name,StringType,true),StructField(wthr_date,StringType,true),StructField(processed_date,TimestampType,true),StructField(year,IntegerType,true),StructField(month,IntegerType,true),StructField(day,IntegerType,true))),List(),None,Map(cloudFiles.format -> parquet, path -> abfss://datasqlwesteurope@sasqlwesteurope.dfs.core.windows.net/incremental-hotel-weather),None), cloudFiles, [address#317, avg_tmpr_c#318, avg_tmpr_f#319, city#320, country#321, geoHash#322, id#323, latitude#324, longitude#325, name#326, wthr_date#327, processed_date#328, year#329, month#330, day#331]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "city: string, wthr_date: string, distinct_hotels: bigint, avg_temperature: double, max_temperature: double, min_temperature: double\n",
      "Aggregate [city#320, wthr_date#327], [city#320, wthr_date#327, approx_count_distinct(id#323, 0.05, 0, 0) AS distinct_hotels#467L, avg(avg_tmpr_c#318) AS avg_temperature#469, max(avg_tmpr_c#318) AS max_temperature#471, min(avg_tmpr_c#318) AS min_temperature#473]\n",
      "+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1a77144e,cloudFiles,List(),Some(StructType(StructField(address,StringType,true),StructField(avg_tmpr_c,DoubleType,true),StructField(avg_tmpr_f,DoubleType,true),StructField(city,StringType,true),StructField(country,StringType,true),StructField(geoHash,StringType,true),StructField(id,StringType,true),StructField(latitude,DoubleType,true),StructField(longitude,DoubleType,true),StructField(name,StringType,true),StructField(wthr_date,StringType,true),StructField(processed_date,TimestampType,true),StructField(year,IntegerType,true),StructField(month,IntegerType,true),StructField(day,IntegerType,true))),List(),None,Map(cloudFiles.format -> parquet, path -> abfss://datasqlwesteurope@sasqlwesteurope.dfs.core.windows.net/incremental-hotel-weather),None), cloudFiles, [address#317, avg_tmpr_c#318, avg_tmpr_f#319, city#320, country#321, geoHash#322, id#323, latitude#324, longitude#325, name#326, wthr_date#327, processed_date#328, year#329, month#330, day#331]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [city#320, wthr_date#327], [city#320, wthr_date#327, approx_count_distinct(id#323, 0.05) AS distinct_hotels#467L, avg(avg_tmpr_c#318) AS avg_temperature#469, max(avg_tmpr_c#318) AS max_temperature#471, min(avg_tmpr_c#318) AS min_temperature#473]\n",
      "+- Project [avg_tmpr_c#318, city#320, id#323, wthr_date#327]\n",
      "   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@1a77144e,cloudFiles,List(),Some(StructType(StructField(address,StringType,true),StructField(avg_tmpr_c,DoubleType,true),StructField(avg_tmpr_f,DoubleType,true),StructField(city,StringType,true),StructField(country,StringType,true),StructField(geoHash,StringType,true),StructField(id,StringType,true),StructField(latitude,DoubleType,true),StructField(longitude,DoubleType,true),StructField(name,StringType,true),StructField(wthr_date,StringType,true),StructField(processed_date,TimestampType,true),StructField(year,IntegerType,true),StructField(month,IntegerType,true),StructField(day,IntegerType,true))),List(),None,Map(cloudFiles.format -> parquet, path -> abfss://datasqlwesteurope@sasqlwesteurope.dfs.core.windows.net/incremental-hotel-weather),None), cloudFiles, [address#317, avg_tmpr_c#318, avg_tmpr_f#319, city#320, country#321, geoHash#322, id#323, latitude#324, longitude#325, name#326, wthr_date#327, processed_date#328, year#329, month#330, day#331]\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[city#320, wthr_date#327], functions=[finalmerge_approx_count_distinct(merge buffer#907) AS approx_count_distinct(id#323, 0.05)#466L, finalmerge_avg(merge sum#899, count#900L) AS avg(avg_tmpr_c#318)#468, finalmerge_max(merge max#902) AS max(avg_tmpr_c#318)#470, finalmerge_min(merge min#904) AS min(avg_tmpr_c#318)#472], output=[city#320, wthr_date#327, distinct_hotels#467L, avg_temperature#469, max_temperature#471, min_temperature#473])\n",
      "+- StateStoreSave [city#320, wthr_date#327], state info [ checkpoint = <unknown>, runId = 276991dd-f9f5-4640-a079-23c873477c2d, opId = 0, ver = 0, numPartitions = 200], Append, 0, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[city#320, wthr_date#327], functions=[merge_approx_count_distinct(merge buffer#907) AS buffer#907, merge_avg(merge sum#899, count#900L) AS (sum#899, count#900L), merge_max(merge max#902) AS max#902, merge_min(merge min#904) AS min#904], output=[city#320, wthr_date#327, buffer#907, sum#899, count#900L, max#902, min#904])\n",
      "      +- StateStoreRestore [city#320, wthr_date#327], state info [ checkpoint = <unknown>, runId = 276991dd-f9f5-4640-a079-23c873477c2d, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "         +- *(2) HashAggregate(keys=[city#320, wthr_date#327], functions=[merge_approx_count_distinct(merge buffer#907) AS buffer#907, merge_avg(merge sum#899, count#900L) AS (sum#899, count#900L), merge_max(merge max#902) AS max#902, merge_min(merge min#904) AS min#904], output=[city#320, wthr_date#327, buffer#907, sum#899, count#900L, max#902, min#904])\n",
      "            +- Exchange hashpartitioning(city#320, wthr_date#327, 200), ENSURE_REQUIREMENTS, [plan_id=114]\n",
      "               +- *(1) HashAggregate(keys=[city#320, wthr_date#327], functions=[partial_approx_count_distinct(id#323, 0.05) AS buffer#907, partial_avg(avg_tmpr_c#318) AS (sum#899, count#900L), partial_max(avg_tmpr_c#318) AS max#902, partial_min(avg_tmpr_c#318) AS min#904], output=[city#320, wthr_date#327, buffer#907, sum#899, count#900L, max#902, min#904])\n",
      "                  +- *(1) Project [avg_tmpr_c#318, city#320, id#323, wthr_date#327]\n",
      "                     +- StreamingRelation cloudFiles, [address#317, avg_tmpr_c#318, avg_tmpr_f#319, city#320, country#321, geoHash#322, id#323, latitude#324, longitude#325, name#326, wthr_date#327, processed_date#328, year#329, month#330, day#331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stream = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .schema(df.schema)\n",
    "    .load(incremental_hotel_weather_path)\n",
    ")\n",
    "\n",
    "result_df = df_stream.groupBy(\"city\", \"wthr_date\").agg(\n",
    "    F.approx_count_distinct(\"id\").alias(\"distinct_hotels\"),\n",
    "    F.avg(\"avg_tmpr_c\").alias(\"avg_temperature\"),\n",
    "    F.max(\"avg_tmpr_c\").alias(\"max_temperature\"),\n",
    "    F.min(\"avg_tmpr_c\").alias(\"min_temperature\"),\n",
    ")\n",
    "\n",
    "result_df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ba106a-9404-4825-9a08-6f4951f255e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    result_df.writeStream.outputMode(\"complete\")\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"result_df_query\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09893247-bc76-4320-9c12-b06ee07e4639",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-682172774778850>:15\u001b[0m\n",
       "\u001b[1;32m     12\u001b[0m top_10_cities_df \u001b[38;5;241m=\u001b[39m unique_cities_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_number()\u001b[38;5;241m.\u001b[39mover(window_spec_top_10))\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Show the resulting DataFrame\u001b[39;00m\n",
       "\u001b[0;32m---> 15\u001b[0m top_10_cities_df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:933\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n",
       "\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
       "\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
       "\u001b[1;32m    926\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m    927\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m    930\u001b[0m         },\n",
       "\u001b[1;32m    931\u001b[0m     )\n",
       "\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    230\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\n",
       "cloudFiles"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-682172774778850>:15\u001b[0m\n\u001b[1;32m     12\u001b[0m top_10_cities_df \u001b[38;5;241m=\u001b[39m unique_cities_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, row_number()\u001b[38;5;241m.\u001b[39mover(window_spec_top_10))\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Show the resulting DataFrame\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m top_10_cities_df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:933\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    926\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_A_BOOLEAN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    927\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m         },\n\u001b[1;32m    931\u001b[0m     )\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\ncloudFiles",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Queries with streaming sources must be executed with writeStream.start();\ncloudFiles",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Window specification to rank cities by distinct hotels and weather date\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(col(\"distinct_hotels\").desc(), col(\"wthr_date\").desc())\n",
    "\n",
    "# Add a row number to each partition\n",
    "result_df_with_row_num = result_df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the top 1 row per city\n",
    "unique_cities_df = result_df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Get the top 10 cities\n",
    "window_spec_top_10 = Window.orderBy(col(\"distinct_hotels\").desc())\n",
    "top_10_cities_df = unique_cities_df.withColumn(\"rank\", row_number().over(window_spec_top_10)).filter(col(\"rank\") <= 10).drop(\"rank\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "top_10_cities_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stream",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
